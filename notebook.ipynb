{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "#import project_tests as tests\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EFTranslator:\n",
    "\n",
    "    def __init__(self, model_name='final_model.h5'):\n",
    "        self.model = load_model(model_name)\n",
    "        self.wv = None\n",
    "        self.english_tokenizer = None\n",
    "        self.french_tokenizer = None\n",
    "        self.english_sentences = None\n",
    "        self.french_sentences = None\n",
    "\n",
    "    # model_name = 'final_model.h5'\n",
    "\n",
    "    # model = load_model(model_name)\n",
    "\n",
    "\n",
    "    # load the data\n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        Load dataset of English and French words\n",
    "        \"\"\"\n",
    "        input_file = os.path.join(path)\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        return data.split('\\n')\n",
    "\n",
    "    def load_en(self, path='data/small_vocab_en.txt'):\n",
    "        self.english_sentences = self.load_data(path)\n",
    "        self.english_sentences = [s.replace('.','').replace(',','').replace('?','').strip().lower() for s in self.english_sentences]\n",
    "\n",
    "    def load_fr(self, path='data/small_vocab_fr.txt'):\n",
    "        self.french_sentences = self.load_data('data/small_vocab_fr.txt')\n",
    "        self.french_sentences = [s.replace('.','').replace(',','').replace('?','').replace('-',' ').lower() for s in self.french_sentences]\n",
    "\n",
    "    def get_counters(self):\n",
    "        self.english_words_counter = collections.Counter([word for sentence in self.english_sentences for word in sentence.split()])\n",
    "        self.french_words_counter = collections.Counter([word for sentence in self.french_sentences for word in sentence.split()])\n",
    "    # print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "    # print('{} unique English words.'.format(len(english_words_counter)))\n",
    "    # print('10 Most common words in the English dataset:')\n",
    "    # print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "    # print()\n",
    "    # print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "    # print('{} unique French words.'.format(len(french_words_counter)))\n",
    "    # print('10 Most common words in the French dataset:')\n",
    "    # print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')\n",
    "\n",
    "\n",
    "    def tokenize(self, x):\n",
    "        '''\n",
    "        Create a tokenizer that can process English words and convert them into numbers\n",
    "        '''\n",
    "        x_tk = Tokenizer(char_level = False)\n",
    "        x_tk.fit_on_texts(x)\n",
    "        return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "    # text_sentences = [\n",
    "    #     'The quick brown fox jumps over the lazy dog .',\n",
    "    #     'By Jove , my quick study of lexicography won a prize .',\n",
    "    #     'This is a short sentence .']\n",
    "\n",
    "    # text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "    # print(text_tokenizer.word_index)\n",
    "    # print()\n",
    "    # for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    #     print('Sequence {} in x'.format(sample_i + 1))\n",
    "    #     print('  Input:  {}'.format(sent))\n",
    "    #     print('  Output: {}'.format(token_sent))\n",
    "\n",
    "\n",
    "    def pad(self, x, length=None):\n",
    "        '''\n",
    "        add padding so that input all has same length - padding is added to the end\n",
    "        '''\n",
    "        if length is None:\n",
    "            length = max([len(sentence) for sentence in x])\n",
    "        return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "    #tests.test_pad(pad)\n",
    "    # Pad Tokenized output\n",
    "    # test_pad = pad(text_tokenized)\n",
    "    # for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    #     print('Sequence {} in x'.format(sample_i + 1))\n",
    "    #     print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    #     print('  Output: {}'.format(pad_sent))\n",
    "\n",
    "\n",
    "    def preprocess(self, x, y):\n",
    "        '''\n",
    "        convert x (English) into numbers representing them, not one hot\n",
    "        convert y (French) into numbers representing the words, not one hot\n",
    "        '''\n",
    "        preprocess_x, x_tk = self.tokenize(x)\n",
    "        preprocess_y, y_tk = self.tokenize(y)\n",
    "        preprocess_x = self.pad(preprocess_x)\n",
    "        preprocess_y = self.pad(preprocess_y)\n",
    "        # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "        preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "        return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "    \n",
    "    # self.preproc_english_sentences, self.preproc_french_sentences, self.english_tokenizer, self.french_tokenizer =\\\n",
    "    #     self.preprocess(english_sentences, french_sentences)\n",
    "\n",
    "    #print('preprocess_french_sentences, y:', preproc_french_sentences[:10])\n",
    "    #print('max french word:', np.max(preproc_french_sentences))\n",
    "    # print('english tokenizer word index', english_tokenizer.word_index)\n",
    "    # print('\\n')\n",
    "\n",
    "    # print('counter items:',english_words_counter.items())\n",
    "\n",
    "    # print('\\n')\n",
    "\n",
    "    def check_balance(self, english_words_counter,english_tokenizer,french_words_counter,french_tokenizer):\n",
    "        '''\n",
    "        determine whether there are missing characters in the english/french tokenizers\n",
    "        '''\n",
    "\n",
    "        a = set([t[0] for t in list(english_words_counter.items())])\n",
    "        b = set(list(english_tokenizer.word_index.keys()))\n",
    "\n",
    "        fa = set([t[0] for t in list(french_words_counter.items())])\n",
    "        fb = set(list(french_tokenizer.word_index.keys()))\n",
    "\n",
    "\n",
    "        # print('list(english_counter.items()):',[t[0] for t in list(english_words_counter.items())])\n",
    "\n",
    "        #print('keys:', list(english_tokenizer.word_index.keys()))\n",
    "\n",
    "        print('Checking word set differences...')\n",
    "        print('english set difference:', a.difference(b))\n",
    "        # english set difference: {'favorite.', 'strawberry.', 'lime.', 'peaches.', '?', 'fruit.', '.', 'lemon.', 'pears.', \n",
    "        # 'grapes.', 'bananas.', 'orange.', 'loved.', 'lemons.', 'pear.', 'grapefruit.', 'grape.', 'apple.', 'oranges.', 'peach.', 'apples.', 'mangoes.', 'liked.', 'banana.', 'limes.', 'mango.', ',', 'strawberries.'}\n",
    "        print('french set difference:', fa.difference(fb))  \n",
    "        # french set difference: {'-elle', 'es-tu', 'aiment-ils', 'est-ce', 'etats-unis', '-', 'préféré.', 'êtes-vous', 'as-tu', 'états-unis', '?', '-ce', 'aimé.', 'États-unis', '.', ',', '-il', '-ils'}\n",
    "        #print('counter difference tokenizer:', set(list(english_words_counter.items())).difference(set(english_tokenizer.word_index.keys())))\n",
    "        return\n",
    "\n",
    "    # self.check_balance(english_words_counter,english_tokenizer,french_words_counter,french_tokenizer)\n",
    "\n",
    "    # max_english_sequence_length = self.preproc_english_sentences.shape[1]\n",
    "    # max_french_sequence_length = self.preproc_french_sentences.shape[1]\n",
    "    # english_vocab_size = len(self.english_tokenizer.word_index)\n",
    "    # french_vocab_size = len(self.french_tokenizer.word_index)\n",
    "    # print('Data Preprocessed')\n",
    "    # print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "    # print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "    # print(\"English vocabulary size:\", english_vocab_size)\n",
    "    # print(\"French vocabulary size:\", french_vocab_size)\n",
    "\n",
    "\n",
    "    def logits_to_text(self, logits, tokenizer):\n",
    "        '''\n",
    "        converting from prediction back to words\n",
    "        '''\n",
    "        index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "        index_to_words[0] = '<PAD>'\n",
    "        return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "    #print('`logits_to_text` function loaded.')\n",
    "\n",
    "\n",
    "    # build tokenizer for model\n",
    "    # save tokenizer??\n",
    "\n",
    "    def predict_sentences(self, s):\n",
    "        x_tk = self.english_tokenizer\n",
    "        y_tk = self.french_tokenizer\n",
    "        model = self.model\n",
    "        \n",
    "        y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "        y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "\n",
    "        sentences = []\n",
    "        for sent in s:\n",
    "            sent = sent.lower()\n",
    "            error = False\n",
    "            # verify each word is in the dictionary\n",
    "            for w in sent.split():\n",
    "                if w not in x_tk.word_index:\n",
    "                    print(w,'not in the English training set')\n",
    "                    error = True\n",
    "                    break\n",
    "            if error:\n",
    "                break\n",
    "\n",
    "            # y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "            # y_id_to_word[0] = '<PAD>'\n",
    "            # sentence = 'he saw a old yellow truck'\n",
    "            # sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "            # sentence = pad_sequences([sentence], maxlen=max_english_sequence_length, padding='post')\n",
    "            # sentences = np.array([sentence[0], x[0]])  # x[0] is a data point from the input\n",
    "\n",
    "            sentence = [x_tk.word_index[word] for word in sent.split()]\n",
    "            print('sent 1',sentence)\n",
    "            sentence = pad_sequences([sentence], maxlen=self.max_english_sequence_length, padding='post')\n",
    "            print('sent 2', sentence)\n",
    "            sentences.append(sentence[0].tolist())\n",
    "\n",
    "        print('sentences')\n",
    "        print(sentences)\n",
    "\n",
    "        predictions = model.predict(np.array(sentences), steps=len(sentences))\n",
    "        print('predictions shape')\n",
    "        print(predictions.shape)\n",
    "\n",
    "        print('generating predictions')\n",
    "        #for i, p in enumerate(predictions[:len(predictions)-1]):\n",
    "        for i in range(len(s)):\n",
    "            p = predictions[i]\n",
    "            print('original')\n",
    "            print(s[i])\n",
    "            print('prediction')\n",
    "            print(' '.join([y_id_to_word[np.argmax(w)] for w in p]))\n",
    "\n",
    "\n",
    "    # What happens if we try to translate a word that we haven't seen before?\n",
    "    # error if word not in dictionary\n",
    "\n",
    "\n",
    "    #self.predict_sentences(to_translate)\n",
    "\n",
    "    def run_preprocess(self):\n",
    "        self.load_en()\n",
    "        self.load_fr()\n",
    "        self.get_counters()\n",
    "        self.preproc_english_sentences, self.preproc_french_sentences, self.english_tokenizer, self.french_tokenizer =\\\n",
    "            self.preprocess(self.english_sentences, self.french_sentences)\n",
    "\n",
    "        # try saving and loading tokenizers\n",
    "        # saving\n",
    "        with open('entokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        with open('frtokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.french_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # loading\n",
    "        with open('entokenizer.pickle', 'rb') as handle:\n",
    "            self.english_tokenizer = pickle.load(handle)\n",
    "        \n",
    "        with open('frtokenizer.pickle', 'rb') as handle:\n",
    "            self.french_tokenizer = pickle.load(handle)\n",
    "        \n",
    "        self.check_balance(self.english_words_counter,self.english_tokenizer,self.french_words_counter,self.french_tokenizer)\n",
    "\n",
    "        self.max_english_sequence_length = self.preproc_english_sentences.shape[1]\n",
    "        self.max_french_sequence_length = self.preproc_french_sentences.shape[1]\n",
    "        self.english_vocab_size = len(self.english_tokenizer.word_index)\n",
    "        self.french_vocab_size = len(self.french_tokenizer.word_index)\n",
    "\n",
    "        self.preprocess_wv()\n",
    "\n",
    "    \n",
    "    def model_final(self, input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "  \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
    "\n",
    "        model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
    "        model.add(RepeatVector(output_sequence_length))\n",
    "        model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
    "        model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "        learning_rate = 0.005\n",
    "        \n",
    "        model.compile(loss = sparse_categorical_crossentropy, \n",
    "                    optimizer = Adam(learning_rate), \n",
    "                    metrics = ['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def create_model(self):\n",
    "        padded_input = self.pad(self.preproc_english_sentences)\n",
    "        #padded_input = self.pad2(self.prepro_eng)\n",
    "        print('padded_input.shape',padded_input.shape)\n",
    "        print('top 5', padded_input[:5])\n",
    "\n",
    "\n",
    "\n",
    "        self.model = self.model_final(padded_input.shape,\n",
    "                    self.preproc_french_sentences.shape[1],\n",
    "                    len(self.english_tokenizer.word_index)+1,\n",
    "                    len(self.french_tokenizer.word_index)+1)\n",
    "        self.model.fit(padded_input, self.preproc_french_sentences, batch_size = 1024, epochs = 17, validation_split = 0.2)\n",
    "\n",
    "    \n",
    "    def load_wv(self,file = 'vw_enfr.wordvecs'):\n",
    "        self.wv = gensim.models.KeyedVectors.load(file)\n",
    "\n",
    "    # I need to change the preprocess in order to manually do the word to vector part\n",
    "\n",
    "    def preprocess_wv(self):\n",
    "        '''\n",
    "        Assume english and french sentences already loaded\n",
    "        Also assume that load wv has been called\n",
    "        '''\n",
    "        if self.wv is None:\n",
    "            self.load_wv()\n",
    "\n",
    "        print('keys:',list(self.wv.vocab))\n",
    "        \n",
    "        prepro_eng = list()\n",
    "        i = 0\n",
    "        for s in self.english_sentences:\n",
    "            i += 1\n",
    "            if i == 10:\n",
    "                break\n",
    "            for w in word_tokenize(s.replace('  ',' ')):\n",
    "                \n",
    "                try:\n",
    "                    wemb = self.wv[w]\n",
    "                    prepro_eng.append(wemb)\n",
    "                except Exception as e:\n",
    "                    print('sentence:',s)\n",
    "                    print('word:',w)\n",
    "\n",
    "        self.prepro_eng = [[self.wv[w] for w in word_tokenize(s.replace('  ',' '))] for s in self.english_sentences]\n",
    "        self.prepro_fr = self.preproc_french_sentences\n",
    "    \n",
    "    def pad2(self, data, length=None):\n",
    "        '''\n",
    "        pad data that has already been transformed into vector embeddings\n",
    "        '''\n",
    "\n",
    "        if length is None:\n",
    "            length = max([len(v) for v in data])  # get length of longest sentence vector\n",
    "\n",
    "        word_dim = len(data[0][0])\n",
    "        print('len of each word:',word_dim)\n",
    "        print('maxlen:',length)\n",
    "\n",
    "        # update each sentence, add vectors of zeros to end of each sentence\n",
    "        # adjusted = [s + [[0 for _ in range(word_dim)] for _ in range(length-len(s))] for s in data]\n",
    "        # return np.array(adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking word set differences...\n",
      "english set difference: set()\n",
      "french set difference: set()\n",
      "keys: ['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', 'and', 'it', 'snowy', 'in', 'april', 'the', 'united', 'states', 'usually', 'chilly', 'july', 'freezing', 'november', 'california', 'march', 'hot', 'june', 'mild', 'cold', 'september', 'your', 'least', 'liked', 'fruit', 'grape', 'but', 'my', 'apple', 'his', 'favorite', 'orange', 'paris', 'relaxing', 'december', 'busy', 'spring', 'never', 'our', 'lemon', 'january', 'warm', 'lime', 'her', 'banana', 'he', 'saw', 'a', 'old', 'yellow', 'truck', 'india', 'rainy', 'that', 'cat', 'was', 'most', 'loved', 'animal', 'dislikes', 'grapefruit', 'limes', 'lemons', 'february', 'china', 'pleasant', 'october', 'wonderful', 'nice', 'summer', 'france', 'may', 'grapes', 'mangoes', 'their', 'mango', 'pear', 'august', 'beautiful', 'apples', 'peaches', 'feared', 'shark', 'wet', 'dry', 'we', 'like', 'oranges', 'they', 'pears', 'she', 'little', 'red', 'winter', 'disliked', 'rusty', 'car', 'strawberries', 'i', 'strawberry', 'bananas', 'going', 'to', 'next', 'plan', 'visit', 'elephants', 'were', 'animals', 'are', 'likes', 'dislike', 'fall', 'driving', 'peach', 'drives', 'blue', 'you', 'bird', 'horses', 'mouse', 'went', 'last', 'horse', 'automobile', 'dogs', 'white', 'elephant', 'black', 'think', 'difficult', 'translate', 'between', 'spanish', 'portuguese', 'big', 'green', 'translating', 'fun', 'where', 'dog', 'why', 'might', 'go', 'this', 'drove', 'shiny', 'sharks', 'monkey', 'how', 'weather', 'lion', 'plans', 'bear', 'rabbit', \"'s\", 'chinese', 'when', 'eiffel', 'tower', 'did', 'grocery', 'store', 'wanted', 'does', 'football', 'field', 'wants', \"n't\", 'snake', 'snakes', 'do', 'easy', 'thinks', 'english', 'french', 'would', 'cats', 'rabbits', 'has', 'been', 'monkeys', 'lake', 'bears', 'school', 'birds', 'want', 'lions', 'am', 'mice', 'have']\n",
      "padded_input.shape (137861, 15)\n",
      "top 5 [[17 23  1  8 67  4 39  7  3  1 55  2 44  0  0]\n",
      " [ 5 20 21  1  9 62  4 43  7  3  1  9 51  2 45]\n",
      " [22  1  9 67  4 38  7  3  1  9 68  2 34  0  0]\n",
      " [ 5 20 21  1  8 64  4 34  7  3  1 57  2 42  0]\n",
      " [29 12 16 13  1  5 82  6 30 12 16  1  5 83  0]]\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/17\n",
      " 15360/110288 [===>..........................] - ETA: 13:59 - loss: 3.5352 - acc: 0.3993"
     ]
    }
   ],
   "source": [
    "model = EFTranslator()\n",
    "model.run_preprocess()\n",
    "model.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}